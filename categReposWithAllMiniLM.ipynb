{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "346153e1-ff55-451b-8bd2-c0e581ee0580",
   "metadata": {},
   "source": [
    "# NOTEBOOK 1 : Découverte et Création des Catégories\n",
    "\n",
    "**Objectif :** Ce notebook analyse un **gros fichier CSV** de dépôts GitHub pour en extraire des catégories de manière non supervisée, **sans saturer la mémoire**.\n",
    "\n",
    "1.  Il lit le CSV par **petits morceaux (chunks)**.\n",
    "2.  Il génère des embeddings pour chaque morceau.\n",
    "3.  Il utilise **`MiniBatchKMeans`** pour apprendre les catégories de manière incrémentale.\n",
    "4.  Il relit le fichier pour assigner les catégories et collecter les textes pertinents.\n",
    "5.  Il nomme et sauvegarde les catégories dans un fichier JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a23eb71-e203-463b-86b2-b5f336f45ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.13/site-packages (2.8.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.13/site-packages (1.7.2)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.13/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.13/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.13/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.13/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.13/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.13/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.13/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.13/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.13/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.13/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.13/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.13/site-packages (from torch) (3.4.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.9.18-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.13/site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.13/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.13/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.13/site-packages (from requests->transformers) (2025.8.3)\n",
      "Downloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading regex-2025.9.18-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (802 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.1/802.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [sentence-transformers]ence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.10 huggingface-hub-0.35.3 regex-2025.9.18 safetensors-0.6.2 sentence-transformers-5.1.1 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy torch transformers sentence-transformers scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "171d32c2-c70c-47a0-b868-07a33074f356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 07:44:11.638402: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation du device : cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "\n",
    "# Machine Learning & Embeddings\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# Fichiers\n",
    "INPUT_CSV_FILE = \"github_data_with_readmes.csv\"\n",
    "OUTPUT_CATEGORIES_FILE = \"github_categories_database.json\"\n",
    "\n",
    "# Paramètres de traitement\n",
    "CHUNK_SIZE = 2000  # Nombre de lignes à traiter en mémoire à la fois. Ajustez selon votre RAM.\n",
    "N_CATEGORIES = 200\n",
    "EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Utilisation du device : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65027f9b-d141-4ca3-8f48-8f4b3e5d4894",
   "metadata": {},
   "source": [
    "### Phase 1 : Entraînement du Modèle de Clustering\n",
    "\n",
    "Ici, nous faisons une première passe sur le fichier. L'objectif est uniquement d'**entraîner le modèle `MiniBatchKMeans`** à reconnaître les `N_CATEGORIES` groupes de dépôts. Nous ne stockons rien en mémoire à part le modèle lui-même."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be70e68-1aec-4477-869e-7393c2b7e987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Entraînement du modèle de clustering ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e780fa5eb844a73a510c75e100d083b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb20e359ea654f7093df25394c5a95ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c466461635468a8fe6ae35406ebc41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aded24d3876449aeb54af425369831b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3f7c3a7e7940adb8a82ea4a64e0d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0b2bb772bd4ffbbe1f95d9fa72c748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87183e12a5a45969293b5ed6579bcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e6649c66a54ede843862a44ba08bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aff1da994ff440fb2239cfff4281fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bdc28f590b4dffbbd900b0b9d29921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b439dbbae54281bc9e92f44ae8d804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8417eccf1f474d13ad9052aec614e895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entraînement du clustering: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"--- Phase 1: Entraînement du modèle de clustering ---\")\n",
    "\n",
    "# 1. Initialiser le modèle de clustering incrémental\n",
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=N_CATEGORIES,\n",
    "    random_state=42,\n",
    "    batch_size=256, # Taille des lots pour l'entraînement interne de KMeans\n",
    "    n_init='auto'\n",
    ")\n",
    "\n",
    "# 2. Initialiser le modèle d'embedding\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL, device=device)\n",
    "\n",
    "# 3. Créer un lecteur de CSV par morceaux\n",
    "csv_reader = pd.read_csv(INPUT_CSV_FILE, chunksize=CHUNK_SIZE, iterator=True)\n",
    "\n",
    "# 4. Boucle d'entraînement sur chaque morceau du fichier\n",
    "for chunk in tqdm(csv_reader, desc=\"Entraînement du clustering\"):\n",
    "    # Préparation du texte pour le lot actuel\n",
    "    chunk['description'] = chunk['description'].fillna('')\n",
    "    chunk['readme_content'] = chunk['readme_content'].fillna('')\n",
    "    chunk['full_text'] = chunk['description'] + ' ' + chunk['readme_content']\n",
    "    \n",
    "    # Filtrer les textes trop courts\n",
    "    chunk = chunk[chunk['full_text'].str.strip().str.len() > 50]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Génération des embeddings pour le lot\n",
    "    embeddings = embedding_model.encode(\n",
    "        chunk['full_text'].tolist(), \n",
    "        show_progress_bar=False, # On a déjà la barre de progression principale\n",
    "        batch_size=128\n",
    "    )\n",
    "    \n",
    "    # Entraînement partiel du modèle sur ce lot\n",
    "    kmeans.partial_fit(embeddings)\n",
    "    \n",
    "    # Libérer la mémoire\n",
    "    del chunk, embeddings\n",
    "    gc.collect()\n",
    "\n",
    "print(\"✅ Modèle de clustering entraîné. Les prototypes des catégories sont prêts.\")\n",
    "category_prototypes = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8104fa14-e508-4078-8462-bb605327c251",
   "metadata": {},
   "source": [
    "### Phase 2 : Assignation des Catégories et Nommage\n",
    "\n",
    "Maintenant que le modèle est entraîné, nous faisons une **deuxième passe** sur le fichier. Cette fois, l'objectif est de :\n",
    "1. Prédire à quelle catégorie appartient chaque dépôt.\n",
    "2. Collecter les textes de chaque catégorie pour pouvoir les nommer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0523693e-3c5f-4332-8b9e-636bc333fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Phase 2: Assignation et nommage des catégories ---\")\n",
    "\n",
    "# 1. Préparer des listes pour stocker les textes de chaque catégorie\n",
    "texts_per_category = [[] for _ in range(N_CATEGORIES)]\n",
    "repos_per_category = [0] * N_CATEGORIES\n",
    "\n",
    "# 2. Recréer le lecteur de CSV pour la deuxième passe\n",
    "csv_reader_pass2 = pd.read_csv(INPUT_CSV_FILE, chunksize=CHUNK_SIZE, iterator=True)\n",
    "\n",
    "for chunk in tqdm(csv_reader_pass2, desc=\"Assignation & Collecte de texte\"):\n",
    "    # Préparation du texte (identique à la phase 1)\n",
    "    chunk['description'] = chunk['description'].fillna('')\n",
    "    chunk['readme_content'] = chunk['readme_content'].fillna('')\n",
    "    chunk['full_text'] = chunk['description'] + ' ' + chunk['readme_content']\n",
    "    chunk = chunk[chunk['full_text'].str.strip().str.len() > 50]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Génération des embeddings\n",
    "    embeddings = embedding_model.encode(chunk['full_text'].tolist(), show_progress_bar=False, batch_size=128)\n",
    "    \n",
    "    # Prédiction des catégories pour ce lot\n",
    "    labels = kmeans.predict(embeddings)\n",
    "    \n",
    "    # Ajout des textes dans les listes correspondantes\n",
    "    for text, label in zip(chunk['full_text'], labels):\n",
    "        # On ne garde qu'un échantillon de textes par catégorie pour ne pas saturer la RAM\n",
    "        if repos_per_category[label] < 200: # Échantillon de 200 dépôts max par catégorie pour le nommage\n",
    "            texts_per_category[label].append(text)\n",
    "        repos_per_category[label] += 1\n",
    "        \n",
    "    del chunk, embeddings, labels\n",
    "    gc.collect()\n",
    "\n",
    "print(\"✅ Textes collectés pour chaque catégorie.\")\n",
    "\n",
    "# 3. Nommage des catégories en utilisant TF-IDF sur les textes collectés\n",
    "print(\"Nommage des catégories...\")\n",
    "categories_database = []\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=2000, max_df=0.8, min_df=2)\n",
    "\n",
    "for i in tqdm(range(N_CATEGORIES), desc=\"Finalisation des catégories\"):\n",
    "    # S'assurer qu'il y a assez de texte pour vectoriser\n",
    "    if len(texts_per_category[i]) < 5:\n",
    "        category_name = f\"Cluster {i} - small_or_generic\"\n",
    "    else:\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts_per_category[i])\n",
    "        terms = vectorizer.get_feature_names_out()\n",
    "        mean_tfidf = np.asarray(tfidf_matrix.mean(axis=0)).ravel()\n",
    "        top_indices = mean_tfidf.argsort()[-5:][::-1]\n",
    "        keywords = [terms[j] for j in top_indices]\n",
    "        category_name = f\"Cluster {i} - {', '.join(keywords)}\"\n",
    "\n",
    "    categories_database.append({\n",
    "        \"category_id\": i,\n",
    "        \"category_name\": category_name,\n",
    "        \"embedding_prototype\": category_prototypes[i].tolist()\n",
    "    })\n",
    "\n",
    "# 4. Sauvegarde finale\n",
    "with open(OUTPUT_CATEGORIES_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(categories_database, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\n✅ Base de {N_CATEGORIES} catégories créée et sauvegardée dans '{OUTPUT_CATEGORIES_FILE}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8747633-4b5a-4a7f-b992-8ef6085c0364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
