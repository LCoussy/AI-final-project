{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0a9e50",
   "metadata": {},
   "source": [
    "# NOTEBOOK 2 : Fine-tuning de RoBERTa pour la Classification de Dépôts\n",
    "\n",
    "**Objectif :** Ce notebook entraîne un classifieur basé sur `roberta-base` pour assigner automatiquement des catégories aux dépôts GitHub.\n",
    "1.  Il charge le CSV des dépôts et la base de catégories (JSON) créée dans le Notebook 1.\n",
    "2.  Il prépare un jeu de données étiquetées en assignant à chaque dépôt la catégorie la plus proche sémantiquement.\n",
    "3.  Il divise les données en un jeu d'entraînement (80%) et un jeu de test (20%).\n",
    "4.  Il fine-tune le modèle `roberta-base` sur les données d'entraînement.\n",
    "5.  Il évalue la performance du modèle sur le jeu de test avec plusieurs métriques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb2149",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy torch transformers datasets scikit-learn sentence-transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba36c40b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Hugging Face\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# Métriques\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# Fichiers d'entrée\n",
    "INPUT_CSV_FILE = \"github_data_with_readmes.csv\"\n",
    "CATEGORIES_FILE = \"github_categories_database.json\"\n",
    "\n",
    "# Modèles\n",
    "BASE_MODEL = 'roberta-base' # Le modèle que nous allons fine-tuner\n",
    "EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2' # Pour l'étiquetage initial\n",
    "\n",
    "# Paramètres\n",
    "OUTPUT_MODEL_DIR = \"./roberta_github_classifier\" # Dossier où sauvegarder le modèle entraîné\n",
    "TEST_SIZE = 0.2 # 20% des données pour le test\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Utilisation du device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc26a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Chargement des données des dépôts\n",
    "df = pd.read_csv(INPUT_CSV_FILE)\n",
    "df['description'] = df['description'].fillna('')\n",
    "df['readme_content'] = df['readme_content'].fillna('')\n",
    "df['full_text'] = df['description'] + ' ' + df['readme_content']\n",
    "df = df[df['full_text'].str.strip().str.len() > 50].reset_index(drop=True)\n",
    "\n",
    "# Chargement de la base de catégories\n",
    "with open(CATEGORIES_FILE, 'r', encoding='utf-8') as f:\n",
    "    categories_db = json.load(f)\n",
    "\n",
    "# Création d'un mapping ID -> Nom pour plus tard\n",
    "id2label = {cat['category_id']: cat['category_name'] for cat in categories_db}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "N_LABELS = len(categories_db)\n",
    "\n",
    "print(f\"{len(df)} dépôts et {N_LABELS} catégories chargés.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13860f92",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Création des étiquettes pour l'entraînement supervisé...\")\n",
    "\n",
    "# 1. Re-générer les embeddings des dépôts (ou les sauvegarder/recharger depuis le Notebook 1)\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL, device=device)\n",
    "repo_embeddings = embedding_model.encode(\n",
    "    df['full_text'].tolist(), \n",
    "    show_progress_bar=True,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# 2. Récupérer les embeddings prototypes de nos catégories\n",
    "category_embeddings = np.array([cat['embedding_prototype'] for cat in categories_db])\n",
    "\n",
    "# 3. Calculer la similarité et assigner le label le plus proche à chaque dépôt\n",
    "print(\"Assignation de la catégorie la plus proche à chaque dépôt...\")\n",
    "similarity_matrix = cosine_similarity(repo_embeddings, category_embeddings)\n",
    "# `argmax` nous donne l'index (et donc l'ID) de la catégorie avec le plus haut score de similarité\n",
    "df['label'] = np.argmax(similarity_matrix, axis=1)\n",
    "\n",
    "# On renomme la colonne de texte pour plus de clarté\n",
    "df.rename(columns={'full_text': 'text'}, inplace=True)\n",
    "\n",
    "print(\"Jeu de données étiquetées prêt.\")\n",
    "df[['full_name', 'label']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b021ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Division en jeux d'entraînement et de test\n",
    "train_df, test_df = train_test_split(df, test_size=TEST_SIZE, random_state=42, stratify=df['label'])\n",
    "\n",
    "# Conversion en objets Dataset de Hugging Face\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n",
    "\n",
    "# Création d'un DatasetDict\n",
    "hf_datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(hf_datasets)\n",
    "\n",
    "# Tokenisation\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = hf_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf540c9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Chargement du modèle pré-entraîné\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL, \n",
    "    num_labels=N_LABELS,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Fonction pour calculer les métriques pendant l'évaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc45e37e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Définition des arguments d'entraînement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_MODEL_DIR,\n",
    "    num_train_epochs=3,              # 3 époques est un bon point de départ\n",
    "    per_device_train_batch_size=16,  # Réduire si vous manquez de VRAM\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",     # Évaluer à la fin de chaque époque\n",
    "    save_strategy=\"epoch\",           # Sauvegarder à la fin de chaque époque\n",
    "    load_best_model_at_end=True,     # Charger le meilleur modèle à la fin\n",
    ")\n",
    "\n",
    "# Création de l'objet Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Lancement du fine-tuning\n",
    "print(\"Lancement du fine-tuning...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68e9896",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nÉvaluation du modèle final sur le jeu de test...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\n--- RÉSULTATS DE L'ÉVALUATION ---\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Sauvegarde du meilleur modèle et du tokenizer\n",
    "trainer.save_model(OUTPUT_MODEL_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_MODEL_DIR)\n",
    "print(f\"\\n✅ Modèle fine-tuné et tokenizer sauvegardés dans '{OUTPUT_MODEL_DIR}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be607d88",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Chargement du pipeline avec notre modèle local fine-tuné\n",
    "classifier = pipeline(\"text-classification\", model=OUTPUT_MODEL_DIR, device=0 if device == 'cuda' else -1)\n",
    "\n",
    "# Exemple de description de dépôt\n",
    "new_repo_description = \"\"\"\n",
    "A new JavaScript library for creating interactive and beautiful data visualizations using D3.js and React. \n",
    "It supports various chart types like bar, line, and pie charts.\n",
    "\"\"\"\n",
    "\n",
    "# Faire une prédiction\n",
    "prediction = classifier(new_repo_description)\n",
    "\n",
    "print(\"\\n--- TEST SUR UN NOUVEL EXEMPLE ---\")\n",
    "print(f\"Description: '{new_repo_description.strip()}'\")\n",
    "print(f\"Catégorie prédite : {prediction[0]['label']} (Score: {prediction[0]['score']:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
